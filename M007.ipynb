{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f29232b2-dc06-498e-a2b0-5de133045697",
   "metadata": {},
   "source": [
    "## Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3fa48a-b0d6-4aeb-9103-6b848a904185",
   "metadata": {},
   "source": [
    "Im Gegensatz zu klassischen ML-Methoden gibt es bei RL kein vorgegebenes Datenset\n",
    "\n",
    "Das Datenset wird während des Prozesses erzeugt, und darauf wird der Algorithmus trainiert\n",
    "\n",
    "Prozess:\n",
    "- Was soll gelernt werden\n",
    "    - Reale Prozesse (Roboter)\n",
    "    - Softwareprozesse (Spiele)\n",
    " \n",
    "Umgebung:\n",
    "- Konkrete Implementation von dem Prozess\n",
    "- Umgebung wird ausgeführt und generiert die Daten\n",
    "\n",
    "Agent:\n",
    "- \"Der Akteur\", welcher den Prozess/Umgebung ausführt\n",
    "- Lernt, generiert das Modell und kann dieses auch verwenden\n",
    "\n",
    "Pakete:\n",
    "- Gymnasium (https://github.com/Farama-Foundation/Gymnasium)\n",
    "- Stable-Baselines 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a8d596f-e7f9-44e0-a12a-bf37b8a7194d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5616dc9-30af-4766-b5b5-5d5b3f93825c",
   "metadata": {},
   "source": [
    "### Vorgegebene Umgebungen\n",
    "\n",
    "Von OpenAI gibt es Demoumgebungen, welche zum testen verwendet werden können\n",
    "\n",
    "Eine davon ist CartPole-v1\n",
    "\n",
    "Es geht darum ein Kart zu bewegen, um eine darauf angebrachte Stange gerade zu halten\n",
    "\n",
    "https://github.com/openai/gym/blob/master/gym/envs/classic_control/cartpole.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f87f5bcc-caaf-4f84-be3c-34da4b3ec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7660b140-bcb2-4c7f-a162-b120abfd2112",
   "metadata": {},
   "source": [
    "### Inhalte einer Umgebung\n",
    "\n",
    "Daten:\n",
    "- Action Space: Was kann der Agent tun? (hier Links bewegen, rechts bewegen)\n",
    "- Observation Space: Die Form der Daten, welche während des Prozesses erzeugt werdn\n",
    "    - Wird vom ML-Algorithmus konsumiert\n",
    " \n",
    "Funktionen:\n",
    "- step: Führt den Prozess um einen Schritt aus\n",
    "    - Bei jedem Step kommt eine Belohnung heraus, welche den Erfolg des Agenten beschreibt\n",
    "- reset: Setzt die Umgebung auf den Anfang zurück, wird für ML verwendet\n",
    "- render: Zeichne die Umgebung\n",
    "- close: Beende die Umgebung, Aufräumen (Fenster schließen, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e53b8-945c-4284-8ef7-f9f5691578c6",
   "metadata": {},
   "source": [
    "### Agent definieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60b1fe27-d5cf-4b02-96b7-20784cb73d5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchgang: 0, Score: 10.0\n",
      "Durchgang: 1, Score: 20.0\n",
      "Durchgang: 2, Score: 9.0\n",
      "Durchgang: 3, Score: 28.0\n",
      "Durchgang: 4, Score: 26.0\n"
     ]
    }
   ],
   "source": [
    "d = 5\n",
    "for x in range(d):\n",
    "    start = env.reset()  # [ 0.0492511 ,  0.04087935, -0.04937328, -0.00469788] (Startpos, StartV, StartPolePos, StartPoleV)\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        env.render()  # Umgebung zeichnen\n",
    "        action = env.action_space.sample()  # Generiere eine Random Action\n",
    "        newState, reward, done, term, info = env.step(action)  # Bei einer Action kommen 5 Werte zurück: Neuer State, Belohnung, Fertig?, Mach Ende?, Info\n",
    "        score += reward\n",
    "    print(f\"Durchgang: {x}, Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1882ca4b-e8e7-44c8-b82f-7c48843a4fda",
   "metadata": {},
   "source": [
    "### RL Modell trainieren\n",
    "\n",
    "Trainingsalgorithmus:\n",
    "- PPO: Proximal Policy Optimization\n",
    "- https://stable-baselines.readthedocs.io/en/master/guide/algos.html\n",
    "\n",
    "Policy:\n",
    "- Bestimmt, wie das unterliegende Modell aufgebaut ist\n",
    "- https://stable-baselines.readthedocs.io/en/master/modules/policies.html\n",
    "\n",
    "DummyVecEnv:\n",
    "- Simulation mehrerer Durchgänge gleichzeitig\n",
    "- Nützlich um Zeit zu sparen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7578adb-1711-4979-a30e-de45f3cb2f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e86c5a36-d207-41d3-b9aa-18be69d46ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b4b2777-65a5-4fc9-9f0d-2de248ad0644",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 687  |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 2    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 335         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008432631 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00201    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.56        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0167     |\n",
      "|    value_loss           | 45.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 290         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011430524 |\n",
      "|    clip_fraction        | 0.0764      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.667      |\n",
      "|    explained_variance   | 0.116       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0204     |\n",
      "|    value_loss           | 31.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 281        |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 29         |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01155111 |\n",
      "|    clip_fraction        | 0.116      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.632     |\n",
      "|    explained_variance   | 0.2        |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 17.2       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0217    |\n",
      "|    value_loss           | 53.9       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 276         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 36          |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006527027 |\n",
      "|    clip_fraction        | 0.0525      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.61       |\n",
      "|    explained_variance   | 0.291       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 29.9        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0141     |\n",
      "|    value_loss           | 64          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 271         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007353957 |\n",
      "|    clip_fraction        | 0.04        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.4         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00849    |\n",
      "|    value_loss           | 56          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 266          |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 53           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0067412015 |\n",
      "|    clip_fraction        | 0.0596       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.576       |\n",
      "|    explained_variance   | 0.695        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 10.2         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.00986     |\n",
      "|    value_loss           | 46.4         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 263          |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 62           |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0083333785 |\n",
      "|    clip_fraction        | 0.0967       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.585       |\n",
      "|    explained_variance   | 0.866        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.39         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.0109      |\n",
      "|    value_loss           | 29.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 261          |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 70           |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0046622087 |\n",
      "|    clip_fraction        | 0.0307       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.568       |\n",
      "|    explained_variance   | 0.795        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.26         |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.00531     |\n",
      "|    value_loss           | 42.9         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 258          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 79           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0144998245 |\n",
      "|    clip_fraction        | 0.105        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.579       |\n",
      "|    explained_variance   | 0.878        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.95         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0107      |\n",
      "|    value_loss           | 33.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 256         |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 87          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007197482 |\n",
      "|    clip_fraction        | 0.0842      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.56       |\n",
      "|    explained_variance   | 0.955       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.71        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00962    |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 96           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062061306 |\n",
      "|    clip_fraction        | 0.0794       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.571       |\n",
      "|    explained_variance   | 0.938        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.56         |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00609     |\n",
      "|    value_loss           | 16.2         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 255          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 104          |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0026322561 |\n",
      "|    clip_fraction        | 0.0116       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.555       |\n",
      "|    explained_variance   | 0.808        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2            |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.00197     |\n",
      "|    value_loss           | 26.3         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200_000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:313\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    310\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dump_logs(iteration)\n\u001b[1;32m--> 313\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m callback\u001b[38;5;241m.\u001b[39mon_training_end()\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:230\u001b[0m, in \u001b[0;36mPPO.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;66;03m# clipped surrogate loss\u001b[39;00m\n\u001b[0;32m    229\u001b[0m policy_loss_1 \u001b[38;5;241m=\u001b[39m advantages \u001b[38;5;241m*\u001b[39m ratio\n\u001b[1;32m--> 230\u001b[0m policy_loss_2 \u001b[38;5;241m=\u001b[39m advantages \u001b[38;5;241m*\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclamp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mratio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclip_range\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclip_range\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    231\u001b[0m policy_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mth\u001b[38;5;241m.\u001b[39mmin(policy_loss_1, policy_loss_2)\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.learn(total_timesteps=200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f08b08d4-9262-4d9a-8d58-9349b6dc81cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"Models/CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e0f1527d-a6c6-4bde-a620-3d0037ac8a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b75c010-7a6e-463e-b2a5-cdeb3a1d71a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(\"Models/CartPole\", env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3ce5e98d-2aeb-41e6-a8a0-24eb70d80b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Durchgang: 0, Score: 1290.0\n",
      "Durchgang: 1, Score: 990.0\n",
      "Durchgang: 2, Score: 1112.0\n",
      "Durchgang: 3, Score: 1122.0\n",
      "Durchgang: 4, Score: 3187.0\n"
     ]
    }
   ],
   "source": [
    "d = 5\n",
    "for x in range(d):\n",
    "    state = env.reset()  # [ 0.0492511 ,  0.04087935, -0.04937328, -0.00469788] (Startpos, StartV, StartPolePos, StartPoleV)\n",
    "    score = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        # env.render()  # Umgebung zeichnen\n",
    "        if len(state) == 2:\n",
    "            action, _ = model.predict(state[0])  # Generiere eine Random Action\n",
    "        else:\n",
    "            action, _ = model.predict(state)\n",
    "        state, reward, done, term, info = env.step(action)  # Bei einer Action kommen 5 Werte zurück: Neuer State, Belohnung, Fertig?, Mach Ende?, Info\n",
    "        score += reward\n",
    "    print(f\"Durchgang: {x}, Score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdab980-a7b6-44dc-b018-e13c2bb097eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
